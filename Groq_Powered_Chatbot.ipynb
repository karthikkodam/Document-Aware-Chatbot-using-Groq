{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfTgQm4NHTzp",
        "outputId": "4a691178-8f63-42d9-8957-3c59361720ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.3/267.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.4/118.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain groq llama-index chromadb sentence-transformers pypdf PyPDF2 langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"groq_api\"] = userdata.get('groq')\n",
        "os.environ[\"groq_api\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BdGw8gOeHhyQ",
        "outputId": "adbbaaaf-78d8-4ca7-ad2a-7853df14ac45"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Loading and Chunking the PDF Document**"
      ],
      "metadata": {
        "id": "jU69zv1NH0JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ISLP_website - Copy.pdf : this document contains the ML and DL topics"
      ],
      "metadata": {
        "id": "zZaalhgITefo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "reader = PdfReader(\"ISLP_website - Copy.pdf\")\n",
        "raw_text = \"\"\n",
        "for page in reader.pages:\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text + \"\\n\""
      ],
      "metadata": {
        "id": "7LnlKMFKHuJT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkLw-zVgZSy2",
        "outputId": "af2a86f9-b7d2-48e6-f5a5-db109da95932"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51521"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=300)\n",
        "docs = splitter.create_documents([raw_text])\n",
        "print(f'no of chunks is : {len(docs)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw9HCVgtHuFy",
        "outputId": "90235a78-0fd7-49a9-f513-baf8b6e44b71"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of chunks is : 487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[450]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoFH6bb2K3T_",
        "outputId": "8e07fa61-c3d8-4b18-d475-b4ef5a05b757"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='426 10. Deep Learning\\nModel # Parameters Mean Abs. Error Test Set R2\\nLinear Regression 20 254.7 0.56\\nLasso 12 252.3 0.51\\nNeural Network 1345 257.4 0.54\\nTABLE 10.2. Prediction results on the Hitters test data for linear models fit\\nby ordinary least squares and lasso, compared to a neural network fit by stochastic\\ngradient descent with dropout regularization.\\nCoefficient Std. error t-statistic p-value\\nIntercept -226.67 86.26 -2.63 0.0103\\nHits 3.06 1.02 3.00 0.0036\\nWalks 0.181 2.04 0.09 0.9294\\nCRuns 0.859 0.12 7.09 <0.0001\\nPutOuts 0.465 0.13 3.60 0.0005\\nTABLE 10.3. Least squares coefficient estimates associated with the regres-\\nsion ofSalaryon four variables chosen by lasso on the Hitters data set. This\\nmodel achieved the best performance on the test data, with a mean absolute error\\nof 224.8. The results reported here were obtained from a regression on the test\\ndata, which was not used in fitting the lasso model.\\nTable10.2compares the results. We see similar performance for all three')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[451]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HB1jmNmLEbX",
        "outputId": "d66f4cac-9d32-4c38-c95e-8a27bf38ec9b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='model achieved the best performance on the test data, with a mean absolute error\\nof 224.8. The results reported here were obtained from a regression on the test\\ndata, which was not used in fitting the lasso model.\\nTable10.2compares the results. We see similar performance for all three\\nmodels. We report the mean absolute error on the test data, as well as\\nthe test R2for each method, which are all respectable (see Exercise 5).\\nWe spent a fair bit of time fiddling with the configuration parameters of\\nthe neural network to achieve these results. It is possible that if we were to\\nspend more time, and got the form and amount of regularization just right,\\nthat we might be able to match or even outperform linear regression and\\nthe lasso. But with great ease we obtained linear models that work well.\\nLinear models are much easier to present and understand than the neural\\nnetwork, which is essentially a black box. The lasso selected 12 of the 19')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqDb6M-PYv7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### To handle large context windows, I used the RecursiveCharacterTextSplitter to break the document into overlapping chunks of 1000 characters each and used overlap of 300 characters."
      ],
      "metadata": {
        "id": "usbLF8KFKgyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LangChain's Recursive Character Text Splitter :\n",
        "* It is used to split long texts into smaller chunks while preserving meaning as much as possible.\n",
        "* It tries to break text at logical boundaries such as paragraphs, sentences, or words—recursively using a list of separators like [comma , fullstop , \\n]\n",
        "\n"
      ],
      "metadata": {
        "id": "MScKHAbnaOen"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mo2AwXYZYwSV"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Embeding the chunks (data) & Storing Chunks in Vector/**"
      ],
      "metadata": {
        "id": "h3Ln7VniIDTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import shutil"
      ],
      "metadata": {
        "id": "V7NkF3CGnabS"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_store = Chroma.from_documents(documents= docs, embedding= embedding_model, persist_directory=\"./chroma_index\")\n",
        "vector_store.persist()"
      ],
      "metadata": {
        "id": "v8XAlEkoMZP0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('chroma_index', 'zip', './chroma_index')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wk-Yy-VJbTYg",
        "outputId": "fa73a3a0-eadf-49d9-8a22-b2ccfd4e0347"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/chroma_index.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3 : Loading Groq LLM and QA model**"
      ],
      "metadata": {
        "id": "ypokPfRjNIDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import Groq\n",
        "\n",
        "llm = Groq(\n",
        "    model=\"llama3-8b-8192\",\n",
        "    api_key=os.getenv(\"groq_api\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "collapsed": true,
        "id": "0vczutofV9OJ",
        "outputId": "07ebbec3-90a1-45ba-9090-250c84283d77"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Groq' from 'langchain.llms' (/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-5f69bed4b985>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m llm = Groq(\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3-8b-8192\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"groq_api\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Groq' from 'langchain.llms' (/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ERROR : ImportError: cannot import name 'Groq' from 'langchain.llms'"
      ],
      "metadata": {
        "id": "O-oD_ULBV9q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### So using ChatOpenAI to connect with Groq's OpenAI-compatible API endpoint for LLM inference"
      ],
      "metadata": {
        "id": "RsjUnI42cPFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model= \"llama-3.3-70b-versatile\",\n",
        "    # model = 'llama3-8b-8192',\n",
        "    temperature= 0,\n",
        "    openai_api_key= os.getenv(\"groq_api\"),\n",
        "    openai_api_base= \"https://api.groq.com/openai/v1\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm= llm,\n",
        "    retriever= vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents= True)"
      ],
      "metadata": {
        "id": "bAYBFtxCO3y1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yk5u38eBhWHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I used Groq AI's 'llama-3.3-70b-versatile' model as the core LLM, leveraging its ultra-fast inference.\n",
        "* Retrieval-Augmented Generation (RAG) was implemented using ChromaDB and semantic embeddings to dynamically fetch relevant chunks and provide contextual answers."
      ],
      "metadata": {
        "id": "A99ZnCZINydx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c8j37MSDeiV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What Generative Models for Classification ?\"\n",
        "response = qa_chain(query)"
      ],
      "metadata": {
        "id": "P8bFRi6mR24F"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(response[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "f57bJ2nLepvZ",
        "outputId": "6968c771-e6d0-4e89-a7dd-4678343d2bab"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "According to the text, Generative Models for Classification involve modeling the distribution of the predictors X separately in each of the response classes, and then using Bayes' theorem to estimate the probabilities Pr(Y=k|X=x). \n\nIn other words, instead of directly modeling the conditional distribution of the response Y given the predictor(s) X, generative models model the joint distribution of X and Y by estimating the distribution of X in each class, and then use Bayes' theorem to obtain the conditional distribution of Y given X.\n\nSome examples of generative models for classification mentioned in the text are:\n\n1. Linear Discriminant Analysis\n2. Quadratic Discriminant Analysis\n3. Naive Bayes \n\nThese models are called \"generative\" because they model the underlying process that generates the data, rather than just modeling the conditional distribution of the response variable given the predictors."
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Source references"
      ],
      "metadata": {
        "id": "FvulyQWcX2KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(response[\"source_documents\"], 1):\n",
        "    print(f\"--- Source {i} ---\")\n",
        "    print(doc.page_content[:300])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgW_MEu_N00u",
        "outputId": "365ecd25-71a0-4647-fb71-f326bde5c607"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Source 1 ---\n",
            "4\n",
            "Classification\n",
            "The linear regression model discussed in Chapter 3assumes that the re-\n",
            "sponse variable Yis quantitative. But in many situations, the response\n",
            "variable is instead qualitative . For example, eye color is qualitative. Of-qualitativeten qualitative variables are referred to as categoric\n",
            "\n",
            "--- Source 2 ---\n",
            "or class. On the other hand, often the methods used for classification first\n",
            "predict the probability that the observation belongs to each of the cate-\n",
            "gories of a qualitative variable, as the basis for making the classification.\n",
            "In this sense they also behave like regression methods.\n",
            "There are many \n",
            "\n",
            "--- Source 3 ---\n",
            "146 4. Classification\n",
            "Thus, rather than estimating coefficients for K−1classes, we actually\n",
            "estimate coefficients for all Kclasses. It is not hard to see that as a result\n",
            "of (4.13), the log odds ratio between the kth andk/primeth classes equals\n",
            "log/parenleftbiggPr(Y=k|X=x)\n",
            "Pr(Y=k/prime|X=x)/parenrig\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **multi-turn conversation**"
      ],
      "metadata": {
        "id": "yj1X7ISkSJpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key= \"chat_history\", return_messages= True)\n",
        "\n",
        "chat_chain = ConversationalRetrievalChain.from_llm(llm= llm,retriever= vector_store.as_retriever(),memory= memory)"
      ],
      "metadata": {
        "id": "DXXAyfz5SC5f"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"What are Linear Discriminant Analysis ?\"\n",
        "chat_response2 = chat_chain.run(query2)\n",
        "Markdown(chat_response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "8X6vbZ4ySgAn",
        "outputId": "a85392a1-5a55-44fd-96a1-37ad96a0fea9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Linear Discriminant Analysis (LDA) is a classification technique that assumes the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all classes. It is used to predict a qualitative response by assigning an observation to the class for which a linear function of the observation is largest.\n\nIn LDA, the classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes' theorem in order to perform predictions. The LDA classifier assigns an observation X = x to the class for which a linear function, known as the discriminant function, is largest.\n\nThe discriminant function in LDA is a linear function of the observation, which is why it is called \"linear\" discriminant analysis. The decision boundary in LDA is a linear line that separates the classes, and all points to one side of the line are assigned to one class, while points to the other side are assigned to another class."
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"what is the difference between the LDA and the QDA?\"\n",
        "chat_response3 = chat_chain.run(query3)\n",
        "Markdown(chat_response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "8AiBw069SnVM",
        "outputId": "2b07b08a-5872-47e6-cf36-d8128217ba94"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The main difference between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) is the assumption they make about the covariance matrix of the classes.\n\nLDA assumes that all classes share a common covariance matrix, which means that the variance and covariance of the features are the same across all classes. This assumption leads to a linear decision boundary.\n\nOn the other hand, QDA assumes that each class has its own covariance matrix, which means that the variance and covariance of the features can differ between classes. This assumption leads to a quadratic decision boundary.\n\nIn other words, LDA is suitable when the classes have similar covariance structures, while QDA is more flexible and can handle classes with different covariance structures. However, QDA can suffer from higher variance and requires a larger training set to estimate the class-specific covariance matrices accurately."
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5olnv2q6twwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Handle Ambiguous or Vague Queries"
      ],
      "metadata": {
        "id": "s3DYDh-WXiVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query4 = \"what is the GEN AI\"\n",
        "chat_response4 = chat_chain.run(query4)\n",
        "Markdown(chat_response4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "QJFatCMiTCRZ",
        "outputId": "85130b32-3baa-41b0-a9d5-2097c32ace9d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know what the \"GEN AI\" refers to. The provided context does not mention \"GEN AI\", and I couldn't find any relevant information about it. If you could provide more context or clarify what you mean by \"GEN AI\", I'll do my best to help."
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EOKPvwMtsS2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Summarization"
      ],
      "metadata": {
        "id": "eDTG8FolsTLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "summarizer = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "summary = summarizer.run(docs[:5])\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "-eO7BBYKUoiO",
        "outputId": "420d232f-0ea7-47e1-a912-fa8c06598dd1"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is a concise summary:\\n\\nThe book \"An Introduction to Statistical Learning, With Applications in Python\" (ISLP) is an introduction to statistical learning, a set of tools for making sense of complex datasets. The book is a Python-based alternative to the popular book \"An Introduction to Statistical Learning, With Applications in R\" (ISLR). ISLP covers the same materials as ISLR, but with labs implemented in Python. The book is intended for advanced undergraduates or master\\'s students in statistics or related quantitative fields, or for individuals in other disciplines who want to use statistical learning tools to analyze their data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using different model [ llama3-8b-8192 ]"
      ],
      "metadata": {
        "id": "syobszSnUxd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    # model=\"llama-3.3-70b-versatile\",\n",
        "    model = 'llama3-8b-8192',\n",
        "    temperature=0,\n",
        "    openai_api_key=os.getenv(\"groq_api\"),\n",
        "    openai_api_base=\"https://api.groq.com/openai/v1\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm = llm,retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}),return_source_documents = True)"
      ],
      "metadata": {
        "id": "7tywVGTvUoUi"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What Generative Models for Classification ?\"\n",
        "response = qa_chain(query)\n",
        "Markdown(response[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "MtyIj8UVUpCN",
        "outputId": "e790d6c7-2065-413b-dfff-b015b37a9f6b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "According to the provided context, Generative Models for Classification are an alternative approach to estimating the probabilities Pr(Y=k|X=x) in logistic regression. In this approach, the distribution of the predictors X is modeled separately in each of the response classes (i.e., for each value of Y), and then Bayes' theorem is used to flip these around into estimates for Pr(Y=k|X=x)."
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the difference between the Multiple Logistic Regression and Multinomial Logistic Regression ?\"\n",
        "response = qa_chain(query)\n",
        "Markdown(response[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "GldOEpBUUr5I",
        "outputId": "32f45af8-f2cb-44cc-cedf-1b179acdc882"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "According to the provided context, Multiple Logistic Regression and Multinomial Logistic Regression are two different types of logistic regression models.\n\nMultiple Logistic Regression is a type of logistic regression that allows for more than two classes for the response variable, but it treats each class as a separate outcome. This means that the model estimates the probability of each class separately, and the coefficients are interpreted as the change in the log odds of each class compared to a baseline class.\n\nOn the other hand, Multinomial Logistic Regression is a type of logistic regression that also allows for more than two classes for the response variable, but it estimates the probability of each class relative to a baseline class. The coefficients in a multinomial logistic regression model are interpreted as the change in the log odds of each class compared to the baseline class.\n\nIn other words, Multiple Logistic Regression estimates the probability of each class separately, while Multinomial Logistic Regression estimates the probability of each class relative to a baseline class."
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLqOLgz2cerT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}